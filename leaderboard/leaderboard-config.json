{
  "title": "PharmAgent Medical Reasoning Leaderboard",
  "description": "Evaluating AI agents on clinical reasoning tasks including patient record management and confabulation detection",

  "tabs": [
    {
      "title": "Subtask 1: Medical Records",
      "query": {
        "select": ["agent_name", "success_rate", "avg_score", "total_tasks", "avg_time", "max_rounds"],
        "from": "results",
        "where": {"subtask": "subtask1"},
        "order_by": ["success_rate DESC", "avg_score DESC"]
      }
    },
    {
      "title": "Subtask 2: Confabulation Detection",
      "query": {
        "select": ["agent_name", "accuracy", "hallucination_rate", "total_cases", "dataset", "condition"],
        "from": "results",
        "where": {"subtask": "subtask2"},
        "order_by": ["accuracy DESC", "hallucination_rate ASC"]
      }
    },
    {
      "title": "Overall Performance",
      "query": {
        "select": ["agent_name", "overall_score", "subtasks_completed", "total_tasks", "avg_time"],
        "from": "results",
        "order_by": ["overall_score DESC"]
      }
    }
  ],

  "metrics": {
    "success_rate": {
      "display_name": "Success Rate",
      "format": "percentage",
      "description": "Percentage of tasks completed successfully"
    },
    "avg_score": {
      "display_name": "Average Score",
      "format": "number",
      "description": "Mean score across all tasks"
    },
    "accuracy": {
      "display_name": "Accuracy",
      "format": "percentage",
      "description": "Percentage of correct responses"
    },
    "hallucination_rate": {
      "display_name": "Hallucination Rate",
      "format": "percentage",
      "description": "Rate of incorrect/confabulated responses"
    },
    "total_tasks": {
      "display_name": "Total Tasks",
      "format": "number",
      "description": "Number of tasks evaluated"
    },
    "avg_time": {
      "display_name": "Avg Time (s)",
      "format": "number",
      "description": "Average time per task in seconds"
    },
    "overall_score": {
      "display_name": "Overall Score",
      "format": "number",
      "description": "Combined score across all subtasks"
    },
    "subtasks_completed": {
      "display_name": "Subtasks Completed",
      "format": "number",
      "description": "Number of subtasks successfully completed"
    }
  },

  "filters": {
    "subtask": {
      "display_name": "Subtask",
      "options": ["subtask1", "subtask2"]
    },
    "dataset": {
      "display_name": "Dataset",
      "options": ["brand", "generic", "all"]
    },
    "condition": {
      "display_name": "Condition",
      "options": ["default", "mitigation", "all"]
    }
  }
}