{
  "name": "PharmAgent Leaderboard",
  "description": "Medical AI Agent Benchmark for evaluating clinical reasoning capabilities",
  "version": "1.0.0",
  "github_repo": "https://github.com/hxwh/AI-PharmD-MedAgentBench",
  "leaderboard_path": "leaderboard",
  "agentbeats_url": "https://agentbeats.dev",
  "scoring": {
    "subtask1": {
      "name": "Medical Record Tasks",
      "metrics": ["score", "success_rate"],
      "primary_metric": "score",
      "description": "Patient lookup, vital signs, lab ordering, consultations"
    },
    "subtask2": {
      "name": "Confabulation Detection",
      "metrics": ["accuracy", "hallucination_rate"],
      "primary_metric": "accuracy",
      "description": "Distinguishing real medications from Pokemon names"
    }
  },
  "queries": [
    {
      "name": "Medical Record Tasks",
      "query": "SELECT participant_id as id, score, success_rate as accuracy, timestamp as completion_time FROM results WHERE subtask = 'subtask1' ORDER BY score DESC"
    },
    {
      "name": "Confabulation Detection",
      "query": "SELECT participant_id as id, accuracy as score, (1.0 - hallucination_rate) as accuracy, timestamp as completion_time FROM results WHERE subtask = 'subtask2' ORDER BY accuracy DESC"
    },
    {
      "name": "Overall Performance",
      "query": "SELECT participant_id as id, AVG(CASE WHEN subtask = 'subtask1' THEN score ELSE accuracy END) as score, COUNT(*) as accuracy, MAX(timestamp) as completion_time FROM results GROUP BY participant_id ORDER BY score DESC"
    }
  ],
  "results_directory": "leaderboard/results/",
  "submissions_directory": "leaderboard/submissions/"
}